{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Q-learning simple NN as function approximation\n",
    "\n",
    "- portfolio grid of size 10 (0 to 1)\n",
    "- 1 hidden neural layers \n",
    "- Improved by Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_ext.csv') #Three stocks (R,X_s,X_b) Without predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of the Tensorflow placeholders and the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize Neural Network and set-up the placeholders\n",
    "tf.reset_default_graph()\n",
    "NN_input = tf.placeholder(shape=[1,6],dtype=tf.float32)\n",
    "NN_weights = tf.Variable(tf.random_uniform([6,10],0,0.01))\n",
    "Q_FA = tf.matmul(NN_input,NN_weights)\n",
    "Q_dropout = tf.nn.dropout(Q_FA,0.2)\n",
    "A_Max = tf.argmax(Q_dropout,1)\n",
    "\n",
    "# Calculate loss for the NN from the Q values\n",
    "Q_Next = tf.placeholder(shape=[1,10],dtype=tf.float32)\n",
    "# loss = tf.reduce_sum(tf.square(Q_Next - Q_FA))\n",
    "loss = tf.reduce_sum(tf.square(Q_Next - Q_dropout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)\n",
    "\n",
    "#Define Action Matrix (Now discrete case) \n",
    "A = np.linspace(0,1,10) # portfolio weights of stocks (1-weight) is the weight in the bonds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the NN function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "Writing away results\n",
      "241\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "gamma = 1 \n",
    "epsilon = 0.1\n",
    "jList = []\n",
    "TWlistTrain = []\n",
    "TWlist = []\n",
    "Index = []\n",
    "MWeights = []\n",
    "# data parsing\n",
    "dates = data['Date']\n",
    "mdata = data[['r','xs','xb','snom','spe','sspr']]\n",
    "mdata.index = pd.DatetimeIndex(dates)\n",
    "n = data.size/4-4\n",
    "periods = 60\n",
    "epochs = 100      # preferred to have a low amount of epochs because otherwise the the same data is used multiple times (usually not the case in stock returns)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(240,int(n-periods-1)):\n",
    "        OptimalWeights = np.zeros(periods-1)\n",
    "        currentK = 0\n",
    "        sess.run(init) # initialize the Neural Network again\n",
    "        print(i)\n",
    "        \n",
    "        while currentK < periods - 1:\n",
    "            #Initilization\n",
    "            NN_data = mdata[0:i+currentK]  # expanding window\n",
    "            # NN_data = mdata[0:i+currentK]   #Expanding window\n",
    "            rAll = 0\n",
    "            currentEpoch = 0\n",
    "            \n",
    "            while currentEpoch < epochs:\n",
    "                #Training of the Q-Network for the data available (with Neural Nets) \n",
    "                for j in range(0,len(NN_data)):\n",
    "                    s = NN_data.iloc[j,:].values.reshape(1,6)\n",
    "                    #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                    a_int,allQ = sess.run([A_Max,Q_FA],feed_dict={NN_input:s})\n",
    "                    a = A[a_int-1]  # -1 because the output neurons are labeled 1 till 101 and it will be an index\n",
    "                    if np.random.rand(1) < epsilon:\n",
    "                        a = random.choice(A)\n",
    "\n",
    "                    #Get new state and reward from environment\n",
    "                    s1 = mdata.iloc[j+i,:].values.reshape(1,6)\n",
    "                    r = (a*s1[0][0] + (1-a)*s1[0][1]) #reward: this is now the wealth gained from this step, but could be other rewards like utility\n",
    "                    Q = sess.run(Q_FA,feed_dict={NN_input:s1})\n",
    "\n",
    "                    #Obtain maxQ' and set our target value for chosen action.\n",
    "                    Q1 = np.max(Q)\n",
    "                    targetQ = allQ\n",
    "                    targetQ[0,a_int] = r + gamma*Q1\n",
    "\n",
    "                    #Train the neural network using target and predicted Q values\n",
    "                    _,W1 = sess.run([updateModel,NN_input],feed_dict={NN_input:s,Q_Next:targetQ})\n",
    "                    rAll += r\n",
    "                    s =  mdata.iloc[j+i,:].values.reshape(1,6)\n",
    "                    if i  > 100:\n",
    "                        # decrease amount of random actions over time in order to improve exploitation rather than exploration\n",
    "                        # only increase exploitation when a good action has been found (otherwise one exploits a bad solution)\n",
    "                        epsilon = 1./((i/50) + 10)\n",
    "                currentEpoch += 1\n",
    "        \n",
    "            # After training now calculate the optimal weights for the K=60 periods to come\n",
    "            s = mdata.iloc[i+periods,:].values.reshape(1,6)\n",
    "            a_int,allQ = sess.run([A_Max,Q_FA],feed_dict={NN_input:s})\n",
    "            aOpt = A[a_int-1]\n",
    "            OptimalWeights[currentK] = aOpt\n",
    "            currentK += 1\n",
    "        \n",
    "        # For insight purposes\n",
    "        MWeights.append(np.mean(OptimalWeights))\n",
    "        TWlist.append(np.exp(sum(OptimalWeights*mdata[i+1:i+currentK+1]['xs'] + (1-OptimalWeights)*mdata[i+1:i+currentK+1]['xb'])))\n",
    "        Index.append(i)\n",
    "        \n",
    "        print('Writing away results')\n",
    "        df = pd.DataFrame({'index date':Index,'TW':TWlist, 'Mean Weights Xs':MWeights})\n",
    "        df.to_excel('Results_NN_g10_e100_do02.xlsx', sheet_name='sheet1', index=False)\n",
    "plt.plot(MWeights)\n",
    "plt.plot(TWlist)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
