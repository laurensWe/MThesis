{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Q-learning simple NN as function approximation\n",
    "\n",
    "- portfolio grid of size 10 (0 to 1)\n",
    "- 1 hidden neural layers \n",
    "- Improved by Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('sim_data_VAR.xlsx') #Three stocks (R,X_s,X_b) Without predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of the Tensorflow placeholders and the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize Neural Network and set-up the placeholders\n",
    "tf.reset_default_graph()\n",
    "NN_input = tf.placeholder(shape=[1,3],dtype=tf.float32)\n",
    "NN_weights = tf.Variable(tf.random_uniform([3,10],0,0.01))\n",
    "b = tf.Variable(np.zeros((1,10)), dtype=tf.float32)\n",
    "Q_FA = tf.tanh(tf.matmul(NN_input,NN_weights) + b)     ## very important \n",
    "Q_dropout = tf.nn.dropout(Q_FA,0.2)\n",
    "A_Max = tf.argmax(Q_dropout,1)\n",
    "\n",
    "# Calculate loss for the NN from the Q values\n",
    "Q_Next = tf.placeholder(shape=[1,10],dtype=tf.float32)\n",
    "# loss = tf.reduce_sum(tf.square(Q_Next - Q_FA))\n",
    "loss = tf.reduce_sum(tf.square(Q_Next - Q_dropout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)\n",
    "\n",
    "#Define Action Matrix (Now discrete case) \n",
    "A = np.linspace(0,1,10) # portfolio weights of stocks (1-weight) is the weight in the bonds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the NN function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524\n",
      "Writing away results\n",
      "525\n",
      "Writing away results\n",
      "526\n",
      "Writing away results\n",
      "527\n",
      "Writing away results\n",
      "528\n",
      "Writing away results\n",
      "529\n",
      "Writing away results\n",
      "530\n",
      "Writing away results\n",
      "531\n",
      "Writing away results\n",
      "532\n",
      "Writing away results\n",
      "533\n",
      "Writing away results\n",
      "534\n",
      "Writing away results\n",
      "535\n",
      "Writing away results\n",
      "536\n",
      "Writing away results\n",
      "537\n",
      "Writing away results\n",
      "538\n",
      "Writing away results\n",
      "539\n",
      "Writing away results\n",
      "540\n",
      "Writing away results\n",
      "541\n",
      "Writing away results\n",
      "542\n",
      "Writing away results\n",
      "543\n",
      "Writing away results\n",
      "544\n",
      "Writing away results\n",
      "545\n",
      "Writing away results\n",
      "546\n",
      "Writing away results\n",
      "547\n",
      "Writing away results\n",
      "548\n",
      "Writing away results\n",
      "549\n",
      "Writing away results\n",
      "550\n",
      "Writing away results\n",
      "551\n",
      "Writing away results\n",
      "552\n",
      "Writing away results\n",
      "553\n",
      "Writing away results\n",
      "554\n",
      "Writing away results\n",
      "555\n",
      "Writing away results\n",
      "556\n",
      "Writing away results\n",
      "557\n",
      "Writing away results\n",
      "558\n",
      "Writing away results\n",
      "559\n",
      "Writing away results\n",
      "560\n",
      "Writing away results\n",
      "561\n",
      "Writing away results\n",
      "562\n",
      "Writing away results\n",
      "563\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function TF_NewBuffer> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Laurens\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[0m__swig_setmethods__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m     \u001b[0m__setattr__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTF_Buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m     \u001b[0m__swig_getmethods__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6f60a898e395>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m                     \u001b[0ms1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#reward: this is now the wealth gained from this step, but could be other rewards like utility\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                     \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_FA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mNN_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                     \u001b[1;31m#Obtain maxQ' and set our target value for chosen action.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Laurens\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    778\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m     \"\"\"\n\u001b[1;32m--> 780\u001b[1;33m     \u001b[0mrun_metadata_ptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       options_ptr = tf_session.TF_NewBufferFromString(\n",
      "\u001b[1;31mSystemError\u001b[0m: <built-in function TF_NewBuffer> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "gamma = 1 \n",
    "epsilon = 0.1\n",
    "jList = []\n",
    "TWlistTrain = []\n",
    "TWlist = []\n",
    "Index = []\n",
    "MWeights = []\n",
    "Turnover = []\n",
    "RU = []\n",
    "\n",
    "# data parsing\n",
    "dates = data['Date']\n",
    "mdata = data[['r','xs','xb']]\n",
    "mdata.index = pd.DatetimeIndex(dates)\n",
    "n = 707\n",
    "periods = 60\n",
    "epochs = 5     # preferred to have a low amount of epochs because otherwise the the same data is used multiple times (usually not the case in stock returns)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(524,int(n-periods-1)):\n",
    "        OptimalWeights = np.zeros(periods-1)\n",
    "        currentK = 0\n",
    "        sess.run(init) # initialize the Neural Network again\n",
    "        print(i)\n",
    "        end = 0\n",
    "        \n",
    "        while currentK < periods - 1:\n",
    "            #Initilization\n",
    "            NN_data = mdata[0:i+currentK]  # Expanding window\n",
    "            currentEpoch = 0\n",
    "            \n",
    "            indexes = np.asarray(range(len(NN_data)))\n",
    "            end = len(NN_data)\n",
    "            np.random.shuffle(indexes)\n",
    "            \n",
    "            while currentEpoch < epochs:\n",
    "                #Training of the Q-Network for the data available (with Neural Nets) \n",
    "                for j in indexes:\n",
    "                    s = NN_data.iloc[j,:].values.reshape(1,3)\n",
    "                    #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                    a_int,allQ = sess.run([A_Max,Q_FA],feed_dict={NN_input:s})\n",
    "                    a = A[a_int-1]  # -1 because the output neurons are labeled 1 till 101 and it will be an index\n",
    "                    if np.random.rand(1) < epsilon:\n",
    "                        a = random.choice(A)\n",
    "\n",
    "                    #Get new state and reward from environment\n",
    "                    s1 = mdata.iloc[j+1,:].values.reshape(1,3)\n",
    "                    r = (a*s1[0][0] + (1-a)*s1[0][1]) #reward: this is now the wealth gained from this step, but could be other rewards like utility\n",
    "                    Q = sess.run(Q_FA,feed_dict={NN_input:s1})\n",
    "\n",
    "                    #Obtain maxQ' and set our target value for chosen action.\n",
    "                    Q1 = np.max(Q)\n",
    "                    targetQ = allQ\n",
    "                    targetQ[0,a_int] = r + gamma*Q1\n",
    "\n",
    "                    #Train the neural network using target and predicted Q values\n",
    "                    _,W1 = sess.run([updateModel,NN_input],feed_dict={NN_input:s,Q_Next:targetQ})\n",
    "                    s =  mdata.iloc[j+1,:].values.reshape(1,3)\n",
    "                    if i  > 100:\n",
    "                        # decrease amount of random actions over time in order to improve exploitation rather than exploration\n",
    "                        # only increase exploitation when a good action has been found (otherwise one exploits a bad solution)\n",
    "                        epsilon = 1./((i/50) + 10)\n",
    "                currentEpoch += 1\n",
    "        \n",
    "            # After training now calculate the optimal weights for the K=60 periods to come\n",
    "            s = mdata.iloc[i+periods,:].values.reshape(1,3)\n",
    "            a_int,allQ = sess.run([A_Max,Q_FA],feed_dict={NN_input:s})\n",
    "            aOpt = A[a_int-1]\n",
    "            OptimalWeights[currentK] = aOpt\n",
    "            currentK += 1\n",
    "            \n",
    "        firstdiff = OptimalWeights[1:] - OptimalWeights[:-1]\n",
    "        # For insight purposes\n",
    "        MWeights.append(np.mean(OptimalWeights))\n",
    "        TerminalWealth = np.exp(sum(OptimalWeights*mdata[i+1:i+currentK+1]['xs'] + (1-OptimalWeights)*mdata[i+1:i+currentK+1]['xb']))\n",
    "        TWlist.append(TerminalWealth)\n",
    "        Index.append(i)\n",
    "        # Turnover\n",
    "        Turnover.append(sum(abs(firstdiff*np.exp(mdata[i+1:i+currentK]['xs'])) + abs((1-firstdiff)*np.exp(mdata[i+1:i+currentK]['xb']))))\n",
    "        # Realized Utility\n",
    "        RU.append((1/(1-5))*pow(TerminalWealth,(1-5)))\n",
    "        \n",
    "        print('Writing away results')\n",
    "        df = pd.DataFrame({'index date':Index,'TW':TWlist, 'Mean Weights Xs':MWeights,'Turnover':Turnover, 'Realized Utility':RU})\n",
    "        df.to_excel('Results_NN_VAR_e5_2.xlsx', sheet_name='sheet1', index=False)\n",
    "print('Done!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
