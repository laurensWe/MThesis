{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Q-learning simple NN as function approximation\n",
    "\n",
    "- portfolio grid of size 10 (0 to 1)\n",
    "- 1 hidden neural layers \n",
    "- Improved by Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('sim_data_CER.xlsx') #Three stocks (R,X_s,X_b) Without predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of the Tensorflow placeholders and the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize Neural Network and set-up the placeholders\n",
    "tf.reset_default_graph()\n",
    "NN_input = tf.placeholder(shape=[1,3],dtype=tf.float32)\n",
    "NN_weights = tf.Variable(tf.random_uniform([3,10],0,0.01))\n",
    "b = tf.Variable(np.zeros((1,10)), dtype=tf.float32)\n",
    "Q_FA = tf.tanh(tf.matmul(NN_input,NN_weights) + b)     ## very important \n",
    "Q_dropout = tf.nn.dropout(Q_FA,0.2)\n",
    "A_Max = tf.argmax(Q_dropout,1)\n",
    "\n",
    "# Calculate loss for the NN from the Q values\n",
    "Q_Next = tf.placeholder(shape=[1,10],dtype=tf.float32)\n",
    "# loss = tf.reduce_sum(tf.square(Q_Next - Q_FA))\n",
    "loss = tf.reduce_sum(tf.square(Q_Next - Q_dropout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)\n",
    "\n",
    "#Define Action Matrix (Now discrete case) \n",
    "A = np.linspace(0,1,10) # portfolio weights of stocks (1-weight) is the weight in the bonds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the NN function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616\n",
      "Writing away results\n",
      "617\n",
      "Writing away results\n",
      "618\n",
      "Writing away results\n",
      "619\n",
      "Writing away results\n",
      "620\n",
      "Writing away results\n",
      "621\n",
      "Writing away results\n",
      "622\n",
      "Writing away results\n",
      "623\n",
      "Writing away results\n",
      "624\n",
      "Writing away results\n",
      "625\n",
      "Writing away results\n",
      "626\n",
      "Writing away results\n",
      "627\n",
      "Writing away results\n",
      "628\n",
      "Writing away results\n",
      "629\n",
      "Writing away results\n",
      "630\n",
      "Writing away results\n",
      "631\n",
      "Writing away results\n",
      "632\n",
      "Writing away results\n",
      "633\n",
      "Writing away results\n",
      "634\n",
      "Writing away results\n",
      "635\n",
      "Writing away results\n",
      "636\n",
      "Writing away results\n",
      "637\n",
      "Writing away results\n",
      "638\n",
      "Writing away results\n",
      "639\n",
      "Writing away results\n",
      "640\n",
      "Writing away results\n",
      "641\n",
      "Writing away results\n",
      "642\n",
      "Writing away results\n",
      "643\n",
      "Writing away results\n",
      "644\n",
      "Writing away results\n",
      "645\n",
      "Writing away results\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "gamma = 1 \n",
    "epsilon = 0.1\n",
    "jList = []\n",
    "TWlistTrain = []\n",
    "TWlist = []\n",
    "Index = []\n",
    "MWeights = []\n",
    "Turnover = []\n",
    "RU = []\n",
    "\n",
    "# data parsing\n",
    "dates = data['Date']\n",
    "mdata = data[['r','xs','xb']]\n",
    "mdata.index = pd.DatetimeIndex(dates)\n",
    "n = 707\n",
    "periods = 60\n",
    "epochs = 5     # preferred to have a low amount of epochs because otherwise the the same data is used multiple times (usually not the case in stock returns)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(616,int(n-periods-1)):\n",
    "        OptimalWeights = np.zeros(periods-1)\n",
    "        currentK = 0\n",
    "        sess.run(init) # initialize the Neural Network again\n",
    "        print(i)\n",
    "        end = 0\n",
    "        \n",
    "        while currentK < periods - 1:\n",
    "            #Initilization\n",
    "            NN_data = mdata[0:i+currentK]  # Expanding window\n",
    "            currentEpoch = 0\n",
    "            \n",
    "            indexes = np.asarray(range(len(NN_data)))\n",
    "            end = len(NN_data)\n",
    "            np.random.shuffle(indexes)\n",
    "            \n",
    "            while currentEpoch < epochs:\n",
    "                #Training of the Q-Network for the data available (with Neural Nets) \n",
    "                for j in indexes:\n",
    "                    s = NN_data.iloc[j,:].values.reshape(1,3)\n",
    "                    #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                    a_int,allQ = sess.run([A_Max,Q_FA],feed_dict={NN_input:s})\n",
    "                    a = A[a_int-1]  # -1 because the output neurons are labeled 1 till 101 and it will be an index\n",
    "                    if np.random.rand(1) < epsilon:\n",
    "                        a = random.choice(A)\n",
    "\n",
    "                    #Get new state and reward from environment\n",
    "                    s1 = mdata.iloc[j+1,:].values.reshape(1,3)\n",
    "                    r = (a*s1[0][0] + (1-a)*s1[0][1]) #reward: this is now the wealth gained from this step, but could be other rewards like utility\n",
    "                    Q = sess.run(Q_FA,feed_dict={NN_input:s1})\n",
    "\n",
    "                    #Obtain maxQ' and set our target value for chosen action.\n",
    "                    Q1 = np.max(Q)\n",
    "                    targetQ = allQ\n",
    "                    targetQ[0,a_int] = r + gamma*Q1\n",
    "\n",
    "                    #Train the neural network using target and predicted Q values\n",
    "                    _,W1 = sess.run([updateModel,NN_input],feed_dict={NN_input:s,Q_Next:targetQ})\n",
    "                    s =  mdata.iloc[j+1,:].values.reshape(1,3)\n",
    "                    if i  > 100:\n",
    "                        # decrease amount of random actions over time in order to improve exploitation rather than exploration\n",
    "                        # only increase exploitation when a good action has been found (otherwise one exploits a bad solution)\n",
    "                        epsilon = 1./((i/50) + 10)\n",
    "                currentEpoch += 1\n",
    "        \n",
    "            # After training now calculate the optimal weights for the K=60 periods to come\n",
    "            s = mdata.iloc[i+periods,:].values.reshape(1,3)\n",
    "            a_int,allQ = sess.run([A_Max,Q_FA],feed_dict={NN_input:s})\n",
    "            aOpt = A[a_int-1]\n",
    "            OptimalWeights[currentK] = aOpt\n",
    "            currentK += 1\n",
    "            \n",
    "        firstdiff = OptimalWeights[1:] - OptimalWeights[:-1]\n",
    "        # For insight purposes\n",
    "        MWeights.append(np.mean(OptimalWeights))\n",
    "        TerminalWealth = np.exp(sum(OptimalWeights*mdata[i+1:i+currentK+1]['xs'] + (1-OptimalWeights)*mdata[i+1:i+currentK+1]['xb']))\n",
    "        TWlist.append(TerminalWealth)\n",
    "        Index.append(i)\n",
    "        # Turnover\n",
    "        Turnover.append(sum(abs(firstdiff*np.exp(mdata[i+1:i+currentK]['xs'])) + abs((1-firstdiff)*np.exp(mdata[i+1:i+currentK]['xb']))))\n",
    "        # Realized Utility\n",
    "        RU.append((1/(1-5))*pow(TerminalWealth,(1-5)))\n",
    "        \n",
    "        print('Writing away results')\n",
    "        df = pd.DataFrame({'index date':Index,'TW':TWlist, 'Mean Weights Xs':MWeights,'Turnover':Turnover, 'Realized Utility':RU})\n",
    "        df.to_excel('Results_NN_CER_e5_4.xlsx', sheet_name='sheet1', index=False)\n",
    "print('Done!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
