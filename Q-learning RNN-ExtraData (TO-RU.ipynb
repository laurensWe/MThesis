{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reinforcement Q-learning with RNN as function approximation\n",
    "\n",
    "- portfolio grid of size 10 (0 to 1)\n",
    "- 2 hidden neural layers with the first one being the recurrent layer (also has the weights of the previous 3 states as input)\n",
    "- Improved by Dropout \n",
    "- Transaction costs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import real data\n",
    "mdata = pd.read_csv('data_ext.csv') #Three stocks (R,X_s,X_b) Without predictors\n",
    "mdata = np.array(mdata[['r','xs','xb','snom','spe','sspr']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "series_length = 15                                           \n",
    "batch_size = 1                                              \n",
    "truncated_backprop_length = series_length//batch_size       # How many previous stock returns to take as input for the model\n",
    "state_size = 4                                              # Number of nodes in the first hidden layer\n",
    "num_classes = 10                                            # Number of classes to predict (10 actions possible so 10 classes)\n",
    "echo_step = 3                                               # How many previous states the neural network takes as input\n",
    "num_stocks = 6                                              # Amount of stocks into consideration\n",
    "gamma = 1                                                   # Discount factor of future Q-values\n",
    "epsilon = 0.1                                               # For the randomization of actions\n",
    "# n = mdata.size/7-4                                          # Length of the total data\n",
    "n = 707\n",
    "periods = 60                                                # How many periods in the future to predict\n",
    "epochs = 5                                                 # Amount of iterations to train the Neural Network\n",
    "TC = 0                                                      # Percentage of transaction costs\n",
    "dropout_prob = (1 - 0.5)                                    # Percentage of neuron nodes to keep in the Network using dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of the Tensorflow placeholders and the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize Recurrent Neural Network and set-up the placeholders\n",
    "tf.reset_default_graph()\n",
    "NN_input = tf.placeholder(tf.float32, [num_stocks, batch_size, truncated_backprop_length])\n",
    "init_state = tf.placeholder(tf.float32, [num_stocks, batch_size, state_size])\n",
    "Q_Next = tf.placeholder(tf.float32, [truncated_backprop_length, batch_size, num_classes])\n",
    "\n",
    "# Weights and biases\n",
    "W = tf.Variable(np.random.rand(num_stocks, state_size+1, state_size), dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((num_stocks,1,state_size)), dtype=tf.float32)\n",
    "W2 = tf.Variable(np.random.rand(num_stocks, state_size, num_classes),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((num_stocks, 1,num_classes)), dtype=tf.float32)\n",
    "\n",
    "inputs_series = tf.unstack(NN_input, axis=2)\n",
    "labels_series = tf.unstack(Q_Next, axis=0)\n",
    "\n",
    "#Forward pass\n",
    "current_state = init_state\n",
    "states_series = []\n",
    "\n",
    "for current_input in inputs_series:\n",
    "    current_input = tf.reshape(current_input, [num_stocks,batch_size, 1])\n",
    "    input_and_state_concatenated = tf.concat([current_input, current_state],axis=2)  # Increasing number of columns\n",
    "    next_state = tf.tanh(tf.matmul(input_and_state_concatenated,W) + b)  # Broadcasted addition\n",
    "    dropout_state = tf.nn.dropout(next_state,dropout_prob)     # DROPOUT\n",
    "    states_series.append(dropout_state)\n",
    "    current_state = next_state\n",
    "\n",
    "#calculate loss\n",
    "Q_FA = [tf.matmul(state, W2) + b2 for state in states_series]\n",
    "dropout_Q = tf.nn.dropout(Q_FA,dropout_prob)    # DROPOUT\n",
    "Q_FA = tf.reduce_sum(dropout_Q,axis=1)\n",
    "A_Max = tf.argmax(Q_FA[-1],1) # only use the latest Q of the RNN for the determination of the optimal weights\n",
    "Q_series = tf.unstack(Q_FA, axis=0)\n",
    "\n",
    "# Calculate loss for the NN from the Q values\n",
    "losses = [ abs(logits - labels) for logits, labels in zip(Q_series,labels_series)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "# loss = tf.reduce_sum(tf.square(Q_Next - Q_FA))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(total_loss)\n",
    "\n",
    "#Define Action Matrix (Now discrete case) \n",
    "A = np.linspace(0,1,10) # portfolio weights of stocks (1-weight) is the weight in the bonds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the RNN function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2c6fa5f9c8d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m                     \u001b[1;31m#Train the neural network using target and predicted Q values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_current_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_total_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdateModel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNN_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mNN_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mQ_Next\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0m_current_state\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m                     \u001b[0mepoch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_total_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mcurrentEpoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Laurens\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Laurens\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Laurens\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\Laurens\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Laurens\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialization for insight series\n",
    "TWlist = []\n",
    "Index = []\n",
    "MWeights = []\n",
    "Turnover = []\n",
    "RU = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(515,n-periods-1):\n",
    "        OptimalWeights = np.zeros(periods-1)\n",
    "        currentK = 0;\n",
    "        print(i)\n",
    "        sess.run(tf.global_variables_initializer()) # initialize the Neural Network again\n",
    "        end = 0\n",
    "        \n",
    "        while currentK < periods - 1:\n",
    "            #Initilization\n",
    "            NN_data = mdata[0:i+currentK]  # rolling window\n",
    "            #initialize an empty hidden state\n",
    "            _current_state = np.zeros((num_stocks, batch_size, state_size))\n",
    "            # NN_data = mdata[0:i+currentK]   #Expanding window\n",
    "            rAll = 0\n",
    "            currentEpoch = 0\n",
    "            \n",
    "            while currentEpoch < epochs:\n",
    "                a_old = 0\n",
    "                epoch_loss = []\n",
    "                \n",
    "                # Randomizing of the data by the indexes\n",
    "                indexes = np.asarray(range(0,len(NN_data)-truncated_backprop_length-1))\n",
    "                end = len(NN_data)-truncated_backprop_length-1\n",
    "                np.random.shuffle(indexes)\n",
    "                \n",
    "                #Training of the Q-Network for the data available (with Neural Nets) \n",
    "                for j in indexes:\n",
    "                    s = NN_data[j:j+truncated_backprop_length,0:num_stocks].reshape(num_stocks,batch_size,truncated_backprop_length)\n",
    "                    #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                    a_int,allQ = sess.run([A_Max,Q_FA],feed_dict={NN_input:s, init_state:_current_state})\n",
    "                    a = A[a_int-1]  # -1 because the output neurons are labeled 1 till 101 and it will be an index\n",
    "                    if np.random.rand(1) < epsilon:\n",
    "                        a = random.choice(A)\n",
    "\n",
    "                    #Get new state and reward from environment\n",
    "                    s1 = NN_data[j+truncated_backprop_length:j+truncated_backprop_length+1,1:4]\n",
    "#                   s1 = mdata[i+currentK+j+truncated_backprop_length+1,1:4]\n",
    "                    r = (a*s1[0][0] + (1-a)*s1[0][1] - TC*abs(a-a_old)) #reward: this is now the wealth gained from this step, but could be other rewards like utility\n",
    "                    \n",
    "                    ## idea: ADD also exp(s1) to the transaction costs\n",
    "                    a_old = a\n",
    "                    s1 = NN_data[j+1:j+truncated_backprop_length+1,0:num_stocks].reshape(num_stocks,batch_size,truncated_backprop_length)\n",
    "                    Q = sess.run(Q_FA,feed_dict={NN_input:s1, init_state:_current_state})\n",
    "                    \n",
    "                    #Obtain maxQ' and set our target value for chosen action.\n",
    "                    Q1 = np.max(Q[-1])\n",
    "                    targetQ = allQ\n",
    "                    targetQ[-1,0,a_int] = r + gamma*Q1\n",
    "            \n",
    "                    ### idea: Print the losses \n",
    "        \n",
    "                    #Train the neural network using target and predicted Q values\n",
    "                    s = s1\n",
    "                    _,_current_state,W1,_total_loss = sess.run([updateModel,current_state,NN_input,total_loss],feed_dict={NN_input:s,Q_Next:targetQ, init_state:_current_state})\n",
    "                    epoch_loss.append(_total_loss)\n",
    "                currentEpoch += 1\n",
    "#                 print(sum(epoch_loss))\n",
    "                    \n",
    "            # After training now calculate the optimal weights for the K=60 periods to come\n",
    "            s1 = NN_data[end:end+truncated_backprop_length,0:num_stocks].reshape(num_stocks,batch_size,truncated_backprop_length)\n",
    "            a_int,allQ = sess.run([A_Max,Q_FA],feed_dict={NN_input:s, init_state:_current_state})\n",
    "            aOpt = A[a_int-1]\n",
    "            OptimalWeights[currentK] = aOpt\n",
    "            currentK += 1\n",
    "        \n",
    "        firstdiff = OptimalWeights[1:] - OptimalWeights[:-1]\n",
    "        # For insight purposes\n",
    "        MWeights.append(np.mean(OptimalWeights))\n",
    "#         x = mdata[i+currentK+j+truncated_backprop_length+1,1:4]\n",
    "        x = mdata[end + truncated_backprop_length - currentK:end+truncated_backprop_length,1:4]\n",
    "        TerminalWealth = np.exp(sum(OptimalWeights*x[:,0] + (1-OptimalWeights)*x[:,1]))\n",
    "        TWlist.append(TerminalWealth)\n",
    "        Index.append(i)\n",
    "        # Turnover\n",
    "        Turnover.append(sum(abs(firstdiff*np.exp(mdata[i+1:i+currentK,1])) + abs((1-firstdiff)*np.exp(mdata[i+1:i+currentK,2]))))\n",
    "        # Realized Utility\n",
    "        RU.append((1/(1-5))*pow(TerminalWealth,(1-5)))\n",
    "        \n",
    "        print('Writing away results')\n",
    "        df = pd.DataFrame({'index date':Index,'TW':TWlist, 'Mean Weights Xs':MWeights,'Turnover':Turnover, 'Realized Utility':RU})\n",
    "        df.to_excel('Results_RNN_g10_e5_TORU_3.xlsx', sheet_name='sheet1', index=False)\n",
    "        \n",
    "    # close session\n",
    "plt.plot(MWeights)\n",
    "plt.plot(TWlist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
