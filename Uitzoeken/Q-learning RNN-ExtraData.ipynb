{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reinforcement Q-learning with RNN as function approximation\n",
    "\n",
    "- portfolio grid of size 10 (0 to 1)\n",
    "- 2 hidden neural layers with the first one being the recurrent layer (also has the weights of the previous 3 states as input)\n",
    "- Improved by Dropout \n",
    "- Transaction costs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import real data\n",
    "mdata = pd.read_csv('data_ext.csv') #Three stocks (R,X_s,X_b) Without predictors\n",
    "mdata = np.array(mdata[['r','xs','xb','snom','spe','sspr']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "series_length = 15                                           \n",
    "batch_size = 1                                              \n",
    "truncated_backprop_length = series_length//batch_size       # How many previous stock returns to take as input for the model\n",
    "state_size = 4                                              # Number of nodes in the first hidden layer\n",
    "num_classes = 10                                            # Number of classes to predict (10 actions possible so 10 classes)\n",
    "echo_step = 3                                               # How many previous states the neural network takes as input\n",
    "num_stocks = 6                                              # Amount of stocks into consideration\n",
    "gamma = 1                                                   # Discount factor of future Q-values\n",
    "epsilon = 0.1                                               # For the randomization of actions\n",
    "# n = mdata.size/7-4                                          # Length of the total data\n",
    "n = 707\n",
    "periods = 60                                                # How many periods in the future to predict\n",
    "epochs = 10                                                 # Amount of iterations to train the Neural Network\n",
    "TC = 0                                                      # Percentage of transaction costs\n",
    "dropout_prob = (1 - 0.5)                                    # Percentage of neuron nodes to keep in the Network using dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of the Tensorflow placeholders and the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize Recurrent Neural Network and set-up the placeholders\n",
    "tf.reset_default_graph()\n",
    "NN_input = tf.placeholder(tf.float32, [num_stocks, batch_size, truncated_backprop_length])\n",
    "init_state = tf.placeholder(tf.float32, [num_stocks, batch_size, state_size])\n",
    "Q_Next = tf.placeholder(tf.float32, [truncated_backprop_length, batch_size, num_classes])\n",
    "\n",
    "# Weights and biases\n",
    "W = tf.Variable(np.random.rand(num_stocks, state_size+1, state_size), dtype=tf.float32)\n",
    "b = tf.Variable(np.zeros((num_stocks,1,state_size)), dtype=tf.float32)\n",
    "W2 = tf.Variable(np.random.rand(num_stocks, state_size, num_classes),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((num_stocks, 1,num_classes)), dtype=tf.float32)\n",
    "\n",
    "inputs_series = tf.unstack(NN_input, axis=2)\n",
    "labels_series = tf.unstack(Q_Next, axis=0)\n",
    "\n",
    "#Forward pass\n",
    "current_state = init_state\n",
    "states_series = []\n",
    "\n",
    "for current_input in inputs_series:\n",
    "    current_input = tf.reshape(current_input, [num_stocks,batch_size, 1])\n",
    "    input_and_state_concatenated = tf.concat([current_input, current_state],axis=2)  # Increasing number of columns\n",
    "    next_state = tf.tanh(tf.matmul(input_and_state_concatenated,W) + b)  # Broadcasted addition\n",
    "    dropout_state = tf.nn.dropout(next_state,dropout_prob)     # DROPOUT\n",
    "    states_series.append(dropout_state)\n",
    "    current_state = next_state\n",
    "\n",
    "#calculate loss\n",
    "Q_FA = [tf.matmul(state, W2) + b2 for state in states_series]\n",
    "dropout_Q = tf.nn.dropout(Q_FA,dropout_prob)    # DROPOUT\n",
    "Q_FA = tf.reduce_sum(dropout_Q,axis=1)\n",
    "A_Max = tf.argmax(Q_FA[-1],1) # only use the latest Q of the RNN for the determination of the optimal weights\n",
    "Q_series = tf.unstack(Q_FA, axis=0)\n",
    "\n",
    "# Calculate loss for the NN from the Q values\n",
    "losses = [ abs(logits - labels) for logits, labels in zip(Q_series,labels_series)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "# loss = tf.reduce_sum(tf.square(Q_Next - Q_FA))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(total_loss)\n",
    "\n",
    "#Define Action Matrix (Now discrete case) \n",
    "A = np.linspace(0,1,10) # portfolio weights of stocks (1-weight) is the weight in the bonds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the RNN function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376\n",
      "Writing away results\n",
      "377\n",
      "Writing away results\n",
      "378\n",
      "Writing away results\n",
      "379\n",
      "Writing away results\n",
      "380\n",
      "Writing away results\n",
      "381\n",
      "Writing away results\n",
      "382\n",
      "Writing away results\n",
      "383\n",
      "Writing away results\n",
      "384\n",
      "Writing away results\n",
      "385\n",
      "Writing away results\n",
      "386\n",
      "Writing away results\n",
      "387\n",
      "Writing away results\n",
      "388\n",
      "Writing away results\n",
      "389\n",
      "Writing away results\n",
      "390\n",
      "Writing away results\n",
      "391\n",
      "Writing away results\n",
      "392\n",
      "Writing away results\n",
      "393\n",
      "Writing away results\n",
      "394\n",
      "Writing away results\n",
      "395\n",
      "Writing away results\n",
      "396\n",
      "Writing away results\n",
      "397\n",
      "Writing away results\n",
      "398\n",
      "Writing away results\n",
      "399\n",
      "Writing away results\n",
      "400\n",
      "Writing away results\n",
      "401\n",
      "Writing away results\n",
      "402\n",
      "Writing away results\n",
      "403\n",
      "Writing away results\n",
      "404\n",
      "Writing away results\n",
      "405\n",
      "Writing away results\n",
      "406\n",
      "Writing away results\n",
      "407\n",
      "Writing away results\n",
      "408\n",
      "Writing away results\n",
      "409\n",
      "Writing away results\n",
      "410\n",
      "Writing away results\n",
      "411\n",
      "Writing away results\n",
      "412\n",
      "Writing away results\n",
      "413\n",
      "Writing away results\n",
      "414\n",
      "Writing away results\n",
      "415\n",
      "Writing away results\n",
      "416\n",
      "Writing away results\n",
      "417\n",
      "Writing away results\n",
      "418\n",
      "Writing away results\n",
      "419\n",
      "Writing away results\n",
      "420\n",
      "Writing away results\n",
      "421\n",
      "Writing away results\n",
      "422\n",
      "Writing away results\n",
      "423\n",
      "Writing away results\n",
      "424\n",
      "Writing away results\n",
      "425\n",
      "Writing away results\n",
      "426\n",
      "Writing away results\n",
      "427\n",
      "Writing away results\n",
      "428\n",
      "Writing away results\n",
      "429\n",
      "Writing away results\n",
      "430\n",
      "Writing away results\n",
      "431\n",
      "Writing away results\n",
      "432\n",
      "Writing away results\n",
      "433\n",
      "Writing away results\n",
      "434\n",
      "Writing away results\n",
      "435\n",
      "Writing away results\n",
      "436\n",
      "Writing away results\n",
      "437\n",
      "Writing away results\n",
      "438\n",
      "Writing away results\n",
      "439\n",
      "Writing away results\n",
      "440\n",
      "Writing away results\n",
      "441\n",
      "Writing away results\n",
      "442\n",
      "Writing away results\n",
      "443\n",
      "Writing away results\n",
      "444\n",
      "Writing away results\n",
      "445\n",
      "Writing away results\n",
      "446\n",
      "Writing away results\n",
      "447\n",
      "Writing away results\n",
      "448\n",
      "Writing away results\n",
      "449\n",
      "Writing away results\n",
      "450\n",
      "Writing away results\n",
      "451\n",
      "Writing away results\n",
      "452\n",
      "Writing away results\n",
      "453\n",
      "Writing away results\n",
      "454\n",
      "Writing away results\n",
      "455\n",
      "Writing away results\n",
      "456\n",
      "Writing away results\n",
      "457\n",
      "Writing away results\n",
      "458\n",
      "Writing away results\n",
      "459\n",
      "Writing away results\n",
      "460\n",
      "Writing away results\n",
      "461\n",
      "Writing away results\n",
      "462\n",
      "Writing away results\n",
      "463\n",
      "Writing away results\n",
      "464\n",
      "Writing away results\n",
      "465\n",
      "Writing away results\n",
      "466\n",
      "Writing away results\n",
      "467\n",
      "Writing away results\n",
      "468\n",
      "Writing away results\n",
      "469\n",
      "Writing away results\n",
      "470\n",
      "Writing away results\n",
      "471\n",
      "Writing away results\n",
      "472\n",
      "Writing away results\n",
      "473\n",
      "Writing away results\n",
      "474\n",
      "Writing away results\n",
      "475\n",
      "Writing away results\n",
      "476\n",
      "Writing away results\n",
      "477\n",
      "Writing away results\n",
      "478\n",
      "Writing away results\n",
      "479\n",
      "Writing away results\n",
      "480\n",
      "Writing away results\n",
      "481\n",
      "Writing away results\n",
      "482\n",
      "Writing away results\n",
      "483\n",
      "Writing away results\n",
      "484\n",
      "Writing away results\n",
      "485\n",
      "Writing away results\n",
      "486\n",
      "Writing away results\n",
      "487\n",
      "Writing away results\n",
      "488\n",
      "Writing away results\n",
      "489\n",
      "Writing away results\n",
      "490\n",
      "Writing away results\n",
      "491\n",
      "Writing away results\n",
      "492\n",
      "Writing away results\n",
      "493\n"
     ]
    }
   ],
   "source": [
    "# initialization for insight series\n",
    "TWlist = []\n",
    "Index = []\n",
    "MWeights = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(376,n-periods-1):\n",
    "        OptimalWeights = np.zeros(periods-1)\n",
    "        currentK = 0;\n",
    "        print(i)\n",
    "        sess.run(tf.global_variables_initializer()) # initialize the Neural Network again\n",
    "        end = 0\n",
    "        \n",
    "        while currentK < periods - 1:\n",
    "            #Initilization\n",
    "            NN_data = mdata[0:i+currentK]  # rolling window\n",
    "            #initialize an empty hidden state\n",
    "            _current_state = np.zeros((num_stocks, batch_size, state_size))\n",
    "            # NN_data = mdata[0:i+currentK]   #Expanding window\n",
    "            rAll = 0\n",
    "            currentEpoch = 0\n",
    "            \n",
    "            while currentEpoch < epochs:\n",
    "                a_old = 0\n",
    "                epoch_loss = []\n",
    "                \n",
    "                # Randomizing of the data by the indexes\n",
    "                indexes = np.asarray(range(0,len(NN_data)-truncated_backprop_length-1))\n",
    "                end = len(NN_data)-truncated_backprop_length-1\n",
    "                np.random.shuffle(indexes)\n",
    "                \n",
    "                #Training of the Q-Network for the data available (with Neural Nets) \n",
    "                for j in indexes:\n",
    "                    s = NN_data[j:j+truncated_backprop_length,0:num_stocks].reshape(num_stocks,batch_size,truncated_backprop_length)\n",
    "                    #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                    a_int,allQ = sess.run([A_Max,Q_FA],feed_dict={NN_input:s, init_state:_current_state})\n",
    "                    a = A[a_int-1]  # -1 because the output neurons are labeled 1 till 101 and it will be an index\n",
    "                    if np.random.rand(1) < epsilon:\n",
    "                        a = random.choice(A)\n",
    "\n",
    "                    #Get new state and reward from environment\n",
    "                    s1 = NN_data[j+truncated_backprop_length:j+truncated_backprop_length+1,1:4]\n",
    "#                   s1 = mdata[i+currentK+j+truncated_backprop_length+1,1:4]\n",
    "                    r = (a*s1[0][0] + (1-a)*s1[0][1] - TC*abs(a-a_old)) #reward: this is now the wealth gained from this step, but could be other rewards like utility\n",
    "                    \n",
    "                    ## idea: ADD also exp(s1) to the transaction costs\n",
    "                    a_old = a\n",
    "                    s1 = NN_data[j+1:j+truncated_backprop_length+1,0:num_stocks].reshape(num_stocks,batch_size,truncated_backprop_length)\n",
    "                    Q = sess.run(Q_FA,feed_dict={NN_input:s1, init_state:_current_state})\n",
    "                    \n",
    "                    #Obtain maxQ' and set our target value for chosen action.\n",
    "                    Q1 = np.max(Q[-1])\n",
    "                    targetQ = allQ\n",
    "                    targetQ[-1,0,a_int] = r + gamma*Q1\n",
    "            \n",
    "                    ### idea: Print the losses \n",
    "        \n",
    "                    #Train the neural network using target and predicted Q values\n",
    "                    s = s1\n",
    "                    _,_current_state,W1,_total_loss = sess.run([updateModel,current_state,NN_input,total_loss],feed_dict={NN_input:s,Q_Next:targetQ, init_state:_current_state})\n",
    "                    epoch_loss.append(_total_loss)\n",
    "                currentEpoch += 1\n",
    "#                 print(sum(epoch_loss))\n",
    "                    \n",
    "            # After training now calculate the optimal weights for the K=60 periods to come\n",
    "            s1 = NN_data[end:end+truncated_backprop_length,0:num_stocks].reshape(num_stocks,batch_size,truncated_backprop_length)\n",
    "            a_int,allQ = sess.run([A_Max,Q_FA],feed_dict={NN_input:s, init_state:_current_state})\n",
    "            aOpt = A[a_int-1]\n",
    "            OptimalWeights[currentK] = aOpt\n",
    "            currentK += 1\n",
    "            \n",
    "        # For insight purposes\n",
    "        MWeights.append(np.mean(OptimalWeights))\n",
    "#         x = mdata[i+currentK+j+truncated_backprop_length+1,1:4]\n",
    "        x = mdata[end + truncated_backprop_length - currentK:end+truncated_backprop_length,1:4]\n",
    "        TWlist.append(np.exp(sum(OptimalWeights*x[:,0] + (1-OptimalWeights)*x[:,1])))\n",
    "        Index.append(i)\n",
    "        \n",
    "        print('Writing away results')\n",
    "        df = pd.DataFrame({'index date':Index,'TW':TWlist, 'Mean Weights Xs':MWeights})\n",
    "        df.to_excel('Results_RNN_g10_extData_e10=2.xlsx', sheet_name='sheet1', index=False)\n",
    "        \n",
    "    # close session\n",
    "plt.plot(MWeights)\n",
    "plt.plot(TWlist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
